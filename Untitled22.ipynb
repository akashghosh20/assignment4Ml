{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668c3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,precision_recall_fscore_support,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7574e5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses</th>\n",
       "      <th>Parents/Children</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  Siblings/Spouses  Parents/Children     Fare\n",
       "0         0       3    male  22.0                 1                 0   7.2500\n",
       "1         1       1  female  38.0                 1                 0  71.2833\n",
       "2         1       3  female  26.0                 0                 0   7.9250\n",
       "3         1       1  female  35.0                 1                 0  53.1000\n",
       "4         0       3    male  35.0                 0                 0   8.0500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\a\\\\PycharmProjects\\\\pythonProject\\\\titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1caf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without k-fold the metrices are below\n",
      "Accouracy Score is  0.7247191011235955\n",
      "Precision Score is  0.640625\n",
      "Recall Score is  0.6119402985074627\n",
      "F1 Score is  0.6259541984732824\n"
     ]
    }
   ],
   "source": [
    "#Creating a new column named Male and it will give male=True and female=False\n",
    "df[\"Male\"]=df[\"Sex\"]==\"male\"\n",
    "# print(df)\n",
    "\n",
    "\n",
    "#Features defining\n",
    "\n",
    "x = df.drop(columns=[\"Survived\",\"Sex\"],axis=1).values\n",
    "#Target defining\n",
    "y = df[\"Survived\"].values\n",
    "\n",
    "#split the datasets\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=5)\n",
    "\n",
    "#using decissiontreeclassifier model\n",
    "model = DT()\n",
    "model.fit(x_train,y_train)\n",
    "# print(model)\n",
    "y_predic = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(\"Without k-fold the metrices are below\")\n",
    "print(\"Accouracy Score is \",accuracy_score(y_test,y_predic))\n",
    "print(\"Precision Score is \",precision_score(y_test,y_predic))\n",
    "print(\"Recall Score is \",recall_score(y_test,y_predic))\n",
    "print(\"F1 Score is \",f1_score(y_test,y_predic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c33952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After k-fold the metrices are below\n",
      "Accouracy Score is  0.770012061194693\n",
      "Precision Score is  0.7109143466348689\n",
      "Recall Score is  0.6822073274547743\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "for criterion in ['gini','entropy']:\n",
    "#     print(\"Decision Tress - {} \".format(criterion))\n",
    "    accuracy=[]\n",
    "    recall=[]\n",
    "    precision=[]\n",
    "    \n",
    "    for train_index,test_index in kf.split(x):\n",
    "        x_train,x_test = x[train_index],x[test_index]\n",
    "        y_train,y_test = y[train_index],y[test_index]\n",
    "        dt = DT(criterion = criterion)\n",
    "        dt.fit(x_train,y_train)\n",
    "        y_pred = dt.predict(x_test)\n",
    "        accuracy.append(accuracy_score(y_test,y_pred))\n",
    "        precision.append(precision_score(y_test,y_pred))\n",
    "        recall.append(recall_score(y_test,y_pred))\n",
    "print(\"After k-fold the metrices are below\")\n",
    "print(\"Accouracy Score is \",np.mean(accuracy))\n",
    "print(\"Precision Score is \",np.mean(precision))\n",
    "print(\"Recall Score is \",np.mean(recall))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac9c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing decission tree\n",
    "features = ['Pclass','Male','Age','Siblings/Spouses','Parents/Children','Fare']\n",
    "from sklearn.tree import export_graphviz as export\n",
    "#graph objects are stored in dot file\n",
    "dot_file = export(model,feature_names=features)\n",
    "\n",
    "# import graphviz as visual\n",
    "# graph = visual.Source(dot_file)\n",
    "# file = 'dt_titanic'\n",
    "# graph.render(file,format='png',cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2613ddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning our tree the metrices are\n",
      "Accuracy Score is  0.7627118644067796\n",
      "Recall Score is  0.6363636363636364\n",
      "Precision Score is  0.7\n",
      "F1 Score is  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "dt = DT(max_depth=6,min_samples_leaf=10,max_leaf_nodes=20)\n",
    "dt.fit(x_train,y_train)\n",
    "print(\"After pruning our tree the metrices are\")\n",
    "print(\"Accuracy Score is \",accuracy_score(y_test,y_pred))\n",
    "print(\"Recall Score is \",recall_score(y_test,y_pred))\n",
    "print(\"Precision Score is \",precision_score(y_test,y_pred))\n",
    "print(\"F1 Score is \",f1_score(y_test,y_pred))\n",
    "# def draw(model,features,name):\n",
    "#     dot_file = export(model,feature_names=features)\n",
    "#     graph = visual.Source(dot_file)\n",
    "#     graph.render(name,format='png',cleanup=True)\n",
    "#     return 0\n",
    "# draw(dt,features,'dt_titanic_prepruned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cc7ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 10, 'max_leaf_nodes': 15, 'min_samples_leaf': 5}\n",
      "Best Score is  0.8309464863835458\n"
     ]
    }
   ],
   "source": [
    "#grid search\n",
    "from sklearn.model_selection import GridSearchCV as grd\n",
    "\n",
    "param_grid = {\n",
    "    'max_leaf_nodes':[15,20,25,30],\n",
    "    'max_depth':[5,8,10,12],\n",
    "    'min_samples_leaf':[5,10,15]\n",
    "    \n",
    "}\n",
    "gs = grd(dt,param_grid,scoring='accuracy',cv=5)\n",
    "\n",
    "gs.fit(x,y)\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best Score is \", gs.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dc8299c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrices of Logistic Regression Model\n",
      "Accuracy Score is  0.795937281787596\n",
      "Precision Score is  0.7556138968208788\n",
      "Recall Score is  0.6926586178202475\n",
      "Metrices of DT \n",
      "Accuracy Score is  0.7813241922173553\n",
      "Precision Score is  0.7187851472396489\n",
      "Recall Score is  0.7141884209255891\n"
     ]
    }
   ],
   "source": [
    "#comparison between logistic Regression and DT\n",
    "\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "dt_accuracy_scores=[]\n",
    "dt_precision_scores=[]\n",
    "dt_recall_scores=[]\n",
    "lr_accuracy_scores=[]\n",
    "lr_precision_scores=[]\n",
    "lr_recall_scores=[]\n",
    "for train_index,test_index in kf.split(x):\n",
    "    x_train,x_test = x[train_index],x[test_index]\n",
    "    y_train,y_test = y[train_index],y[test_index]\n",
    "    model1 = DT()\n",
    "    model2 = LogisticRegression()\n",
    "    model1.fit(x_train,y_train)\n",
    "    model2.fit(x_train,y_train)\n",
    "    y_prediction = model1.predict(x_test)\n",
    "    y_predlr = model2.predict(x_test)\n",
    "    dt_accuracy_scores.append(accuracy_score(y_test,y_prediction))\n",
    "    dt_precision_scores.append(precision_score(y_test,y_prediction))\n",
    "    dt_recall_scores.append(recall_score(y_test,y_prediction))\n",
    "    lr_accuracy_scores.append(accuracy_score(y_test,y_predlr))\n",
    "    lr_precision_scores.append(precision_score(y_test,y_predlr))\n",
    "    lr_recall_scores.append(recall_score(y_test,y_predlr))\n",
    "print(\"Metrices of Logistic Regression Model\")\n",
    "print(\"Accuracy Score is \",np.mean(lr_accuracy_scores))\n",
    "print(\"Precision Score is \",np.mean(lr_precision_scores))\n",
    "print(\"Recall Score is \",np.mean(lr_recall_scores))\n",
    "print(\"Metrices of DT \")\n",
    "print(\"Accuracy Score is \",np.mean(dt_accuracy_scores))\n",
    "print(\"Precision Score is \",np.mean(dt_precision_scores))\n",
    "print(\"Recall Score is \",np.mean(dt_recall_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab3946d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_recall_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#determine sebsitivity thats the recall and sprcificity r[0] of lgr \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sensitivityLgr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlr_recall_scores\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sensitivityLgr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_recall_scores' is not defined"
     ]
    }
   ],
   "source": [
    "#determine sebsitivity thats the recall and sprcificity r[0] of lgr \n",
    "sensitivityLgr = np.mean(lr_recall_scores)\n",
    "print(sensitivityLgr)\n",
    "def specificityLgr(y_test,y_predlr):\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test,y_predlr)\n",
    "    return r[0]\n",
    "specificityLgr = specificityLgr(y_test,y_pred)\n",
    "\n",
    "\n",
    "#determine sebsitivity thats the recall and sprcificity r[0] of DT\n",
    "\n",
    "sensitivityDt = np.mean(dt_recall_scores)\n",
    "def specificityDt(y_test,y_prediction):\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test,y_prediction)\n",
    "    return r[0]\n",
    "specificityDt = specificityDt(y_test,y_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dee4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import roc_auc_score as auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de170ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Roc curve for Decision tree\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fpr,tpr,thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m(y_test,y_prediction)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSensitivity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-Specificity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_curve' is not defined"
     ]
    }
   ],
   "source": [
    "#Roc curve for Decision tree\n",
    "fpr,tpr,thresholds = roc_curve(y_test,y_prediction)\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.xlabel(\"1-Specificity\")\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1],linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34375555",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predlr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#roc curve for logistic regression\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fprlr,tprlr,thresholds \u001b[38;5;241m=\u001b[39m roc(y_test,\u001b[43my_predlr\u001b[49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSensitivity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-Specificity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_predlr' is not defined"
     ]
    }
   ],
   "source": [
    "#roc curve for logistic regression\n",
    "fprlr,tprlr,thresholds = roc(y_test,y_predlr)\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.xlabel(\"1-Specificity\")\n",
    "plt.plot(fprlr,tprlr)\n",
    "plt.plot([0,1],[0,1],linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f4766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1011c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
